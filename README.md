# 《深 度 学 习 和 计 算 机 视 觉 》 课 程 实 验 三 ： CIFAR-ViT

1. [实验内容](#实验内容)
2. [实验要求](#实验要求)
3. [类不均衡的解决措施](#类不均衡的解决措施)
   - [修改损失函数](#修改损失函数)
   - [数据增强](#数据增强)
4. [ViT的搭建](#vit的搭建)
   - [ViT的基本流程](#vit的基本流程)
   - [测试预训练模型](#测试预训练模型)
   - [构建核心代码](#构建核心代码)
   - [实验结果](#实验结果)
5. [ViT轻量化](#vit轻量化)
   - [DynamicViT](#dynamicvit)
   - [EarlyExitViT](#earlyexitvit)
6. [Swin Transformer](#swin-transformer)
7. [CoAtNet](#coatnet)
8. [实验心得](#实验心得)


## 实验内容
利用 PyTorch 等框架，设计和搭建 ViT 实现 CIFAR-10 数据集分类任 务，分析实验结果；探索降低 ViT 复杂度的方法。

## 实验要求

实现 ViT 构建：

搜索和阅读相关资料与论文。
使用课程提供的 CIFAR-10 数据集完成分类任务。
解决类不均衡问题：

针对训练数据集中的类不均衡现象，尝试解决方法并进行解释。
ViT 轻量化：

在确保分类性能不明显下降的前提下，优化 ViT 模型。
提升训练与推理速度，降低模型参数量。

## 类不均衡的解决措施

### 修改损失函数
#### 方法一
**加权交叉熵**
**机制**: 在标准交叉熵损失 `CE = -y * log(p)` 的基础上，为每个类别 `i` 赋予一个权重 `wi`。`wi` 通常与类别频率成反比（例如 `wi = N_total / N_i`）。
**公式**: `WCE = - &Sigma; (wi * yi * log(pi))`
#### 方法二
Focal Loss
**公式**:
$$ FL(p_t) = - \alpha \cdot (1 - p_t)^\gamma \cdot \log(p_t) $$

**核心**:
- **关键项**: $(1 - p_t)^\gamma$
- $p_t$ 表示模型对正确类别的预测概率（置信度）。
**优点**:
**自动聚焦**: 
Focal Loss 会**忽略**那些已经学得很好的多数类样本。
**自动地**将注意力集中在那些“困难”的（通常是少数类）样本上。

## 数据增强
**Mixup (混合)**：    
    - **原理**：像调鸡尾酒。它将两张图 A 和 B **按一定比例（例如 70%/30%）的透明度叠加**在一起。
    - **标签**：新图片的标签也按**相同比例混合**（例如 `0.7*标签A + 0.3*标签B`）。
    - **效果**：强迫模型在不同类别之间学习一个平滑的过渡，防止“过分自信”。
**CutMix (剪贴)**：    
    - **原理**：像打补丁。它从图 B 上**随机剪切一块区域**，然后**粘贴**到图 A 的相同位置，将其覆盖。
    - **标签**：新图片的标签则**按“补丁”的面积比例**混合（例如 `80%*标签A + 20%*标签B`）。
    - **效果**：强迫模型必须观察整张图片来做决定，而不能只依赖某个局部特征（因为这个特征可能被“补丁”盖住）

**少数类的图片会不断地被“混合”或“粘贴”到多数类的图片上，这**极大地增加**了模型在一个训练周期（Epoch）中“看到”少数类特征的频率，迫使模型必须学会识别它们。

## ViT的搭建

### ViT的基本流程
#### 1.图像分块
**目的**：将2D图像转换为1D序列，作为Transformer的输入。

- **操作**：将输入图像 **`(H, W, C)`** 分割成 **`N`** 个不重叠的块（Patches），每个块尺寸为 **`(P, P)`**。
- **映射**：将每个块展平后，通过一个可训练的线性投影层映射到一个 **`D`** 维的向量空间。此即**块嵌入 **。
#### 2. 添加位置信息
**目的**：为模型注入空间位置信息，因为自注意力机制本身不具有感知位置的能力。
- **操作**：为每个块的位置生成一个与嵌入维度 **`D`** 相同的**可学习的位置编码向量**。
- **融合**：将位置编码向量与对应的块嵌入向量进行**相加**，得到最终的输入序列。
#### 3. 引入分类令牌 ([CLS] Token)

**目的**：提供一个用于聚合全局信息、最终用于分类的专用令牌。
- **操作**：在由图像块嵌入组成的序列 **`(N, D)`** 的开头，拼接一个**可学习的** **`[CLS]`** 令牌，形成长度为 **`N+1`** 的输入序列。

### 4. Transformer 编码 
**目的**：对输入序列进行深层的特征提取和全局上下文建模。
- **结构**：输入序列通过一个由 **`L`** 个相同的**Transformer编码器层**堆叠而成的模块。
- **核心组件 (每层包含)**：
    - **多头自注意力机制 (MSA)**：让序列中的每个令牌（包括 `[CLS]`）与所有其他令牌进行交互，捕获全局依赖关系。
    - **前馈神经网络 (MLP)**：对每个令牌的特征进行非线性变换。
    - **层归一化 与**残差连接：应用于每个子层之前/后，确保训练稳定性。
### 5. 输出与分类 
**目的**：根据学习到的全局表示执行分类任务。
- **操作**：仅取最终输出序列中**第一个位置**的向量，即 **`[CLS]` 令牌**对应的 **`D`** 维输出。
- **分类**：将该向量通过一个小型的**多层感知机 (MLP Head)**，最终得到分类预测结果。
### 测试预训练模型

再进行自己的实验之前，实验者使用vit_b_16预训练模型配合mlp的分类头在数据集上进行迁移学习测试，结果如下。

### 构建核心代码

```python

class ViT(nn.Module):

def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, in_channels=3, dropout=0.1, emb_dropout=0.1):

super().__init__()

  

self.patch_embedding = PatchEmbedding(image_size, patch_size, in_channels, dim)

self.dropout = nn.Dropout(emb_dropout)

self.transformer = nn.Sequential(*[

TransformerBlock(dim, heads, mlp_dim, dropout) for _ in range(depth)

])

self.mlp_head = nn.Sequential(

nn.LayerNorm(dim),

nn.Linear(dim, num_classes)

)

def forward(self, img):
x = self.patch_embedding(img)
x = self.dropout(x)
x = self.transformer(x)
cls_output = x[:, 0]
return self.mlp_head(cls_output)
```

### 实验结果

| Experiment | Img Size | Patch Size | Dim  | Depth | Heads | MLP Dims | Accuracy |
| ---------- | -------- | ---------- | ---- | ----- | ----- | -------- | -------- |
| vit_v1.1   | 32       | 4          | 512  | 6     | 8     | 2048     | 0.8080   |
| vit_v1.2   | 32       | 4          | 512  | 6     | 16    | 2048     | 0.5427   |
| vit_v1.3   | 32       | 4          | 640  | 6     | 10    | 2560     | 0.8563   |
| vit_v1.4   | 32       | 4          | 640  | 10    | 10    | 2560     | 0.8565   |
| vit_v1.5   | 32       | 4          | 896  | 8     | 14    | 3584     | 0.7368   |
| vit_v1.6   | 32       | 4          | 896  | 12    | 14    | 3584     | 0.8464   |
| vit_v1.7   | 224      | 16         | 768  | 8     | 12    | 3072     | 0.8975   |
| vit_v1.8   | 32       | 16         | 768  | 8     | 12    | 3072     | 0.4290   |
| vit_v1.9   | 32       | 4          | 768  | 8     | 12    | 3072     | 0.8667   |
| vit_v1.10  | 32       | 4          | 768  | 12    | 12    | 3072     | 0.8538   |
| vit_v1.11  | 32       | 4          | 768  | 4     | 12    | 3072     | 0.6006   |
| vit_v1.12  | 32       | 4          | 896  | 10    | 14    | 3584     | 0.8462   |
| vit_v1.13  | 32       | 4          | 1024 | 12    | 16    | 4096     | 0.8343   |


| Experiment | Img Size | Patch Size | Dim  | Depth | Heads | MLP Dims | Total Parameters | Total FLOPs (G) |
| ---------- | -------- | ---------- | ---- | ----- | ----- | -------- | ---------------- | --------------- |
| vit_v1.1   | 32       | 4          | 512  | 6     | 8     | 2048     | 18.94 M          | 1.23            |
| vit_v1.2   | 32       | 4          | 512  | 6     | 16    | 2048     | 18.94 M          | 1.23            |
| vit_v1.3   | 32       | 4          | 640  | 6     | 10    | 2560     | 29.57 M          | 1.92            |
| vit_v1.4   | 32       | 4          | 640  | 10    | 10    | 2560     | 49.26 M          | 3.20            |
| vit_v1.5   | 32       | 4          | 896  | 8     | 14    | 3584     | 77.20 M          | 5.02            |
| vit_v1.6   | 32       | 4          | 896  | 12    | 14    | 3584     | 115.77 M         | 7.52            |
| vit_v1.7   | 224      | 16         | 768  | 8     | 12    | 3072     | 57.28 M          | 11.28           |
| vit_v1.8   | 32       | 16         | 768  | 8     | 12    | 3072     | 57.28 M          | 0.29            |
| vit_v1.9   | 32       | 4          | 768  | 8     | 12    | 3072     | 56.73 M          | 3.69            |
| vit_v1.10  | 32       | 4          | 768  | 12    | 12    | 3072     | 85.07 M          | 5.53            |
| vit_v1.11  | 32       | 4          | 768  | 4     | 12    | 3072     | 28.39 M          | 1.84            |
| vit_v1.12  | 32       | 4          | 896  | 10    | 14    | 3584     | 96.48 M          | 6.27            |
| vit_v1.13  | 32       | 4          | 1024 | 12    | 16    | 4096     | 151.18 M         | 9.82            |
## ViT轻量化
此阶段，所使用的基础模型为之前在标准ViT阶段表现最好的模型，vit_v1.4，以此预训练模型/或者是专家模型，对此进行轻量化处理。
### DynamicViT

#### 基本原理
标准ViT对所有的Tokens都一视同仁，投入相同的计算量。但事实上，一张图像中不同区域的**信息密度是不同的**。对所有Token进行全程计算是一种资源浪费。
DynamicViT 的思路是：**在模型前向传播的过程中，逐步丢弃那些“不重要”的Token，只对保留下的Token进行后续计算**。这样，越到深层，需要处理的Token越少，计算量自然下降。

实现机制：
 **重要性评分（Importance Scoring）**：
    在特定的层（例如每隔几层之后），引入一个轻量级的**预测模块（通常是一个小型神经网络或线性层+激活函数）。
    该模块为每个Token计算一个**重要性分数。这个分数通常基于Token自身的特征向量。
**Token选择（Token Selection）**：
    - 根据计算出的重要性分数，对所有Token进行排序。
    - 只保留Top-K个分数最高的Token，或者保留分数超过某个阈值的Token。
    - 被判定为“不重要”的Token将被**丢弃**，它们不再参与后续所有层的自注意力计算和MLP计算。
#### 实验效果

DynamicViT
参数量: 49.26M（与原始模型 vit_v1.4 相同）。
计算量 (FLOPs): 从 3.20G 降低到 2.09G，减少约 34.7% 的计算量。
准确率 (Accuracy): 从 0.8565 降低到 0.8158，下降约 4.1%。

### EarlyExitViT
#### 基本原理

标准ViT必须跑完所有层才能得出预测结果。但有些“简单”的样本可能只需要浅层特征就足以被正确分类，无需经过所有复杂的深层变换。让所有样本都“一刀切”地经历全部计算是低效的。

EarlyExitViT 的思路是：**在网络的不同深度设置多个“出口”，每个出口都可以进行预测并输出结果。如果模型在某个浅层出口已经对当前样本做出了高置信度的预测，就可以提前终止计算，直接返回结果**。

**实现机制：**
1. **插入出口**：
    - 在ViT主干网络的多个中间层（例如第4、8、12层）后，插入**分类器**。
    - 这个分类器通常是一个简单的结构，如全局平均池化层 + 全连接层，它以当前层的 CLS Token或所有Token的特征作为输入。
2. **推理过程**：
    - 输入图像，数据依次通过各层。
    - 每到达一个出口，就计算一次**置信度**。
    - 设定一个**置信度阈值。如果当前出口的预测置信度高于阈值，模型就“相信”这个预测结果，立即终止后续计算，并返回该结果。
    - 如果置信度不高，则继续前往下一个更深的出口，直到最后一个出口做出最终预测。
#### 实验效果
参数量: 49.26M（与原始模型相同）。
**平均**计算量 (FLOPs): 从 3.20G 降低到 2.29G，减少约 28.43% 的计算量。
准确率 (Accuracy): 从 0.8565 降低到 0.81889，下降约 3.761%。


## Swin Transformer

### ViT存在的两个主要瓶颈
1. **计算复杂度高**：
ViT将图像分割成固定大小的块，然后对这些块进行全局自注意力计算。其计算复杂度与图像中块的数量呈**二次方关系**（O(n²)）。这意味着当输入图像分辨率很高（如1024x1024）时，块的数量会非常多，计算量和内存占用会变得巨大，使其难以应用于密集预测任务（如目标检测、语义分割）。
2. **缺乏尺度与平移不变性：
ViT从始至终都处理全局的、固定分辨率的特征图，其结构本身**不具备归纳偏置。它没有像CNN那样内置的“局部性”（一个像素只与其邻近像素相关）和“平移等变性”（物体移动后，其特征图也相应移动）的假设。一切都需要从数据中学习，这通常需要极大的数据量。

### Swin Transformer 原理
Swin Transformer通过引入两个核心思想来解决上述问题：
#### 1. 层级式结构

这是Swin Transformer与ViT最根本的不同。

- **ViT**：从输入到输出，特征图的尺寸（Token的数量）是**固定不变**的。
- **Swin**：模仿CNN的设计，构建了一个**金字塔式的层级结构**。
- **过程**：
    1. 开始时，图像被切成小块（如4x4像素），产生许多小尺寸的“特征图”。
    2. 通过“Patch Merging”层，将相邻的2x2小块合并成一个新的大块，同时将其特征维度翻倍。这相当于下采样，将特征图尺寸减半，通道数翻倍。
    3. 经过几个阶段后，特征图的尺寸逐渐减小，通道数逐渐增加，形成了类似CNN的多尺度特征图输出。    
#### 2. 移位窗口自注意力

这是Swin Transformer实现高效计算的核心创新。
- **问题**：全局自注意力计算成本太高。
- **解决方案**：将自注意力计算限制在**一个个不重叠的局部窗口** 内。这样，计算复杂度就从与图像大小的二次方关系变成了**线性关系**（O(n)），因为每个窗口内的token数量是固定的，与图像总大小无关。
- **新问题**：固定的窗口之间没有通信，模型无法获得全局的上下文信息。
- **终极解决方案**：**窗口移位**。
- **过程**：
    1. 在Swin Transformer块的第L层，使用常规的窗口划分（红色实线）计算自注意力。
    2. 在第L+1层，将窗口向右和向下各移动半个窗口的大小（“移位”，黄色虚线）。这个简单的操作彻底打破了原有窗口的边界。
    3. 现在，新窗口由上一层中**不同旧窗口** 的部分组成。通过新窗口内的自注意力计算，上一层不同窗口的信息就实现了交互和融合。
- **优势**：
    - **高效**：仍然只在局部窗口内计算自注意力，保持了线性计算复杂度。
    - **全局性**：通过连续的移位操作，信息实际上可以在整个图像中传播，模型间接地获得了全局建模能力。这巧妙地用连续的局部计算逼近了全局计算的效果。

### Swin Transformer 实验结果

| Img Size | Patch Size | Window Size | Num Heads      |
| -------- | ---------- | ----------- | -------------- |
| 32       | 4          | 4           | [3, 6, 12, 24] |

| Experiment | Embed Dim | Depths       | Parameters | Total FLOPs (G) | Accuracy |
| ---------- | --------- | ------------ | ---------- | --------------- | -------- |
| swint_v0.1 | 96        | [2, 2, 6, 2] | 27.50 M    | 0.09            | 0.824    |
| swint_v0.2 | 192       | [2, 2, 6, 2] | 109.86 M   | 0.36            | 0.8513   |
| swint_v0.3 | 384       | [2, 2, 6, 2] | 439.14 M   | 1.42            | 0.8037   |
| swint_v0.4 | 96        | [2, 4, 6, 2] | 28.39 M    | 0.10            | 0.8497   |
| swint_v0.5 | 96        | [4, 6, 8, 4] | 47.23 M    | 0.16            | 0.8294   |
| swint_v0.6 | 96        | [4, 4, 6, 2] | 28.62 M    | 0.12            | 0.8267   |
| swint_v0.7 | 96        | [4, 4, 4, 2] | 25.07 M    | 0.10            | 0.8253   |
| swint_v0.8 | 96        | [6, 4, 4, 2] | 25.29 M    | 0.12            | 0.8458   |

由上图实验结果表格看出，Swin Transformer 在达到与ViT 类似的 Accuracy 的时候可以 ，Parameters的大小并不规律，但是其Total FLOPs几乎比ViT的低初一个或者几个数量级。


## CoAtNet

### ViT及SwinT等存在的问题

ViT及其早期变体（如Swin）存在的几个核心问题：
1. **样本效率低下与归纳偏置缺失**：
    - **问题**：纯粹的ViT缺乏CNN固有的**归纳偏置（Inductive Bias）**，如平移不变性、局部性和尺度不变性。因此，它在中小型数据集（如ImageNet-1K）上训练时，通常需要非常大量的数据增强和更长的训练周期才能达到与CNN相当的性能，否则容易过拟合。
    - **目标**：CoAtNet希望将CNN的**强大归纳偏置**引入Transformer架构，使其在数据量不那么巨大时也能高效、稳定地训练并表现出色。
2. **局部特征提取的粗糙性**：
    - **问题**：ViT将图像直接分割成块（Patches），这个过程可以看作是一种简单且粗糙的“卷积”，其核大小和步长等于块大小（如16x16）。这种“一击式”的下采样会丢失大量细粒度的局部信息（如边缘、纹理）。
    - **目标**：CoAtNet希望像CNN一样，通过堆叠多个小核卷积层来**逐步、平滑地提取局部特征**，更好地保留细节信息。
3. **全局建模的计算复杂度**：
    - **问题**：虽然Swin通过局部窗口降低了复杂度，但其移位窗口机制增加了实现的复杂性。CoAtNet探索了一种更直接的方式：**只在网络的高层、特征图分辨率已经降低后，才使用计算昂贵的全局自注意力**。
### CoAtNet原理

CoAtNet的核心思想非常直观：**深度融合卷积和自注意力**，让它们各自发挥优势。其原理主要体现在两个方面：
#### 1. 混合模型设计

CoAtNet不是一个简单的卷积+注意力拼接，而是一个从底到顶、由卷积分支和注意力分支**共同参与**的复合块，称为**MBConv**块与**Self-Attention**块的融合。

在这个复合块中，输入会同时经过一个**卷积操作**和一个**自注意力操作**，然后将两个结果相加（求和）作为输出。  
`Output = Conv(Input) + Attention(Input)`

这种设计让模型在每一层都能同时捕获**局部特征（由卷积负责）** 和**全局交互（由注意力负责）**。

#### 2. 纵向分层结构

这是CoAtNet架构设计中最关键、最精彩的部分。它不是一个简单的堆叠，而是遵循一个“**从卷积到注意力**”的渐进原则，构建了五个阶段：
- **阶段1和2（S0, S1）：纯卷积阶段**
    - 使用标准的**MBConv**块（带SE注意力机制的反残差块）。
    - **目的**：在**高分辨率**的底层特征图上，充分发挥卷积在**提取局部细节特征**方面的高效性和强大归纳偏置。这相当于一个非常强大的“特征提取器”，为后面的全局建模准备了丰富且高质量的特征。
- **阶段3和4（S2, S3）：混合阶段**
    - 使用上面介绍的**融合块（MBConv + Self-Attention）**。
    - **目的**：随着特征图分辨率逐步降低（通过下采样），计算成本下降。此时开始引入**全局自注意力机制**，与卷积提供的局部特征进行互补和增强。模型在此阶段同时具备强大的局部和全局建模能力。
- **阶段5（S4）：纯注意力阶段**
    - 使用标准的**Transformer Self-Attention**块（类似于ViT块）。
    - **目的**：在网络的**最深层、特征图分辨率最低**时，完全依赖强大的全局自注意力来整合所有信息，做出最高层次的语义抽象。

### CoAtNet实验结果

| Heads | Transformer Dim Head | MBConv Expand Ratios | MBConv Kernel Sizes | Img Size |
| ----- | -------------------- | -------------------- | ------------------- | -------- |
| 8     | 32                   | [4, 4]               | [3, 3]              | 32       |


| Experiment   | Block Types          | Dims                | Depths       | Total Parameters | Total FLOPs (G) | Accuracy |
| ------------ | -------------------- | ------------------- | ------------ | ---------------- | --------------- | -------- |
| coatnet_v0.1 | ['C', 'C', 'T', 'T'] | [64, 128, 256, 512] | [2, 2, 4, 2] | 8.85 M           | 0.10            | 0.8763   |
| coatnet_v0.2 | ['C', 'C', 'T', 'T'] | [64, 128, 256, 512] | [3, 3, 5, 2] | 11.55 M          | 0.16            | 0.8641   |
| coatnet_v0.3 | ['C', 'C', 'T', 'T'] | [96, 192, 384, 768] | [3, 4, 6, 3] | 31.86 M          | 0.40            | 0.8115   |
| coatnet_v0.4 | ['C', 'C', 'T', 'T'] | [64, 128, 256, 512] | [2, 2, 2, 2] | 8.85 M           | 0.10            | 0.8035   |

由以上表格可以看出，CoAtNet在达到相同甚至更高的Accuracy的时候，其Total Parameters和 FLOPs 都低与 SwinT和ViT，并且其更容易达到更好的Accuracy。
## 实验心得


本次实验收获颇丰。通过从零开始搭建、训练和调试Vision Transformer (ViT) 及其变体，我对Transformer架构在计算机视觉领域的应用有了更深刻的理解。

1. **ViT 对超参数和小数据集的敏感性：** 在基础ViT的调参实验中 (v1.1至v1.13)，我发现ViT模型对超参数（如 `Dim`, `Depth`, `Heads`）的设置极为敏感。例如，`vit_v1.2` 仅将 `Heads` 从8增加到16，准确率便从 80.8% 骤降至 54.27%，显示了不当配置导致的训练崩溃。 一个关键的发现是 **Patch Size** 对CIFAR-10这种低分辨率 (32x32) 数据集的决定性影响。实验 `vit_v1.8` (Patch Size=16) 几乎完全无法收敛 (42.90% 准确率)，因为 32x32 的图像被切成 16x16 的块后，序列长度仅为 4，模型无法学到有效特征。而 `vit_v1.9` (Patch Size=4) 则能达到 86.67% 的高准确率。这证明了为低分辨率图像选择合适的 Patch Size 以保证足够的序列长度是ViT应用的前提。
    
2. **轻量化方法的利弊权衡：** 在ViT轻量化探索中，DynamicViT 实现了计算量 (FLOPs) 约 34.7% 的显著下降。这验证了在推理过程中动态丢弃冗余Token的有效性。然而，这也带来了约 4.1% 的精度损失 (从 85.65% 降至 81.58%)。这表明，虽然轻量化策略能提升效率，但在当前实现中，这种效率提升是以牺牲一定性能为代价的，如何在两者间找到最佳平衡点是未来值得研究的方向。
    
3. **卷积归纳偏置的不可替代性：** 本次实验最深刻的体会来自于 Swin Transformer 和 CoAtNet 的对比。
    
    - **Swin Transformer** 通过引入“层级结构”和“移位窗口自注意力”，模仿了CNN的多尺度特征提取，并解决了ViT的全局注意力二次方复杂度问题。实验结果显示，Swin (如 `swint_v0.2`) 能以**低几个数量级**的 FLOPs (0.36G) 达到与ViT (如 `vit_v1.4`, 3.20G) 相当甚至更高的准确率 (85.13%)。
        
    - **CoAtNet** 的表现最为惊艳。它将CNN的归纳偏置（通过MBConv块）与Transformer的全局建模能力（通过Self-Attention）在架构中进行了深度融合和分阶段（Staging）设计。实验 `coatnet_v0.1` 以仅 **8.85M** 的参数量和 **0.10G** 的FLOPs，达到了全场最佳的 **87.63%** 准确率，全面超越了ViT和Swin。
        

**总结：** 本次实验清晰地揭示了，对于CIFAR-10这类中小型数据集，缺乏归纳偏置的纯ViT模型在效率和性能上均不占优势。而Swin Transformer，特别是CoAtNet，通过巧妙地将卷积的局部性、平移不变性等优秀特性与Transformer的全局依赖建模能力相结合，才是实现高性能、高效率视觉模型的关键路径。


本次试验所有代码以及试验结果记录都已经上传至github仓库 https://github.com/HughWang-wzy/vit_model
